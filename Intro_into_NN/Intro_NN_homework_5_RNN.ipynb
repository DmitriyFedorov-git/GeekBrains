{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUIkx5ycGXbu"
   },
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb либо нейронной сети работающей airline-passengers(она прилагается вместе с датасетом к уроку в виде отдельного скрипта) так, чтобы улучшить ее точность. Приложите анализ.</li>\n",
    "    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n",
    "    <li>* Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM</li>\n",
    "    <li>* Предложите свои варианты решения проблемы исчезающего градиента в RNN</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "Cgz22l9Aqi-r",
    "outputId": "b9c45f09-7bc0-49f6-801d-296a58dfba5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n",
      "Построение модели...\n",
      "Процесс обучения...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 6s 238us/step - loss: 0.6865 - accuracy: 0.6061 - val_loss: 0.6690 - val_accuracy: 0.6889\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 5s 218us/step - loss: 0.5953 - accuracy: 0.7512 - val_loss: 0.5095 - val_accuracy: 0.7908\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 5s 213us/step - loss: 0.4497 - accuracy: 0.8206 - val_loss: 0.3847 - val_accuracy: 0.8346\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 5s 215us/step - loss: 0.3536 - accuracy: 0.8557 - val_loss: 0.3661 - val_accuracy: 0.8427\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 5s 217us/step - loss: 0.2919 - accuracy: 0.8878 - val_loss: 0.3457 - val_accuracy: 0.8599\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 5s 217us/step - loss: 0.2531 - accuracy: 0.9060 - val_loss: 0.3483 - val_accuracy: 0.8498\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 6s 235us/step - loss: 0.2211 - accuracy: 0.9203 - val_loss: 0.3500 - val_accuracy: 0.8556\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 6s 224us/step - loss: 0.1998 - accuracy: 0.9309 - val_loss: 0.3634 - val_accuracy: 0.8477\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 6s 221us/step - loss: 0.1837 - accuracy: 0.9370 - val_loss: 0.3805 - val_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 5s 218us/step - loss: 0.1641 - accuracy: 0.9437 - val_loss: 0.3968 - val_accuracy: 0.8480\n",
      "25000/25000 [==============================] - 0s 18us/step\n",
      "Результат при тестировании: 0.3968168890476227\n",
      "Тестовая точность: 0.8479599952697754\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "maxlen = 200\n",
    "batch_size = 2000 # увеличьте значение для ускорения обучения\n",
    "\n",
    "print('Загрузка данных...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'тренировочные последовательности')\n",
    "print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "print('Pad последовательности (примеров в x единицу времени)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Построение модели...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#lr_schedule  = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.6)\n",
    "\n",
    "# стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='adam',\n",
    "              #optimizer = SGD(learning_rate=0.01, momentum=0.9),\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Процесс обучения...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10, # увеличьте при необходимости\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Результат при тестировании:', score)\n",
    "print('Тестовая точность:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3O0RomMr8g4"
   },
   "source": [
    "1. batch_size увеличен в 40 раз для повышения скорости обучения модели\n",
    "2. Количество эпох увеличено до 10, но при этои точность на валидационной выборке достигла максимума на 5-й эпохе. После этого начало увеличиваться переобучение.\n",
    "3. при уменьшении maxlen скорость обучения выросла, но результирующая точность модели снизилась, так как модель тренируется на более коротких последовательностях слов\n",
    "4. оптимальное значение maxlen равно 200. Если брать больше, то точность не увеличится, но время обучения возрастет, так как сеть обрабатывает слишком длинные последовательности\n",
    "5. при уменьшении max_features модель становится менее точной, так как много слов, важных для определения типа отзыва, отбрасываются, так как встречаются не часто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Guf6OFtwHPi0",
    "outputId": "c5258f28-9885-455c-d997-5f5c0a356aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qoAzR05aqjCA",
    "outputId": "c79625d8-900b-4be0-8cd9-f69922cec297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 7s 46us/step - loss: 2.7125\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 2.1057\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 40us/step - loss: 1.9149\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 40us/step - loss: 1.7790\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.6713\n",
      "Генерация из посева: can, said the duches\n",
      "can, said the duchess the satter the satter the satter the satter the satter the satter the satter the satter the satter==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.5810\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.4993\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.4283\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.3624\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.3029\n",
      "Генерация из посева: -tm and future gener\n",
      "-tm and future general of the works and and and and and and and and and and and and and and and and and and and and and ==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.2466\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.1903\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.1370\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.0817\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 1.0262\n",
      "Генерация из посева: air of white kid glo\n",
      "air of white kid gloves and the other side. the froged right to be to herself, and the other side. the froged right to b==================================================\n",
      "Итерация #: 3\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 40us/step - loss: 0.9697\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 40us/step - loss: 0.9124\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.8560\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.7947\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 40us/step - loss: 0.7338\n",
      "Генерация из посева:  make out which were\n",
      " make out which were the rest of the gryphon. alice seented to be sound of the sente. in the same thing what you well en==================================================\n",
      "Итерация #: 4\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.6736\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.6169\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.5612\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.5065\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.4569\n",
      "Генерация из посева: rforming, distributi\n",
      "rforming, distributing or drapping the gryphon, and the gome was not a boing on the sease near one fange lates one of th==================================================\n",
      "Итерация #: 5\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.4107\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.3683\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.3298\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.2984\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.2699\n",
      "Генерация из посева: os to go down the ch\n",
      "os to go down the chimner, has he didna pleased to find that i bet a came appearing to herself about the wryton-s allar ==================================================\n",
      "Итерация #: 6\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.2460\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.2260\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.2092\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1949\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1813\n",
      "Генерация из посева: not allow disclaimer\n",
      "not allow disclaimers you can exccat till she had beginning. thats nothend diskes. on, you sio, you fong the little dis,==================================================\n",
      "Итерация #: 7\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1722\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1648\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1552\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1490\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1455\n",
      "Генерация из посева: and join the dance. \n",
      "and join the dance. will you, wont you, woll you just befied it, said the king. the wires it turned uply weat in their c==================================================\n",
      "Итерация #: 8\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1398\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1360\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1325\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1286\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1260\n",
      "Генерация из посева:  idea what a delight\n",
      " idea what a delightful thing a low hardly providing vorce, foun atce and a fort. off to the knave of a fell had my see ==================================================\n",
      "Итерация #: 9\n",
      "Epoch 1/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1261\n",
      "Epoch 2/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1204\n",
      "Epoch 3/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1193\n",
      "Epoch 4/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1170\n",
      "Epoch 5/5\n",
      "158763/158763 [==============================] - 6s 39us/step - loss: 0.1159\n",
      "Генерация из посева: it in a low, hurried\n",
      "it in a low, hurried tone. he looked anxiously it orfocket, mase mo, that they work only yes; but to here? the hatter sh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# построчное чтение из примера с текстом \n",
    "with open(\"/content/drive/My Drive/alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 20, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 1024, 512\n",
    "NUM_ITERATIONS = 10 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 5\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9x8t8HqIo1J"
   },
   "source": [
    "1. Было увеличино количество итераций, чтобы модель успела научиться должным образом\n",
    "2. При увеличении HIDDEN_SIZE модель обучается быстрее (loss быстрее снижается), так как модель становится более мощной\n",
    "3. При увеличении SEQLEN модель обучается быстрее, но каждая эпоха по времени длится дольше, так как модель обучается на более длинной последовательности слов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AS_f8791RqoT"
   },
   "source": [
    "Варианты решения проблемы исчезающего градиента в RNN:\n",
    "1. Использование архитектур наподобие LSTM;\n",
    "2. Инициализировать веса так, чтобы минимизировать вероятность затухания градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zb2DNj4iSjzw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "metodich5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
