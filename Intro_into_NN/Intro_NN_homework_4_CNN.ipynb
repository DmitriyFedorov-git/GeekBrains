{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZcdsD-8X4q9"
   },
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробовать улучшить точность распознования образов cifar 10 сверточной нейронной сетью, рассмотренной на уроке. Приложить анализ с описанием того, что улучшает работу нейронной сети и что ухудшает.\n",
    "    </li>\n",
    "    <li>Описать также в анализе какие необоходимо внести изменения  в получившуюся у вас нейронную сеть если бы ей нужно было работать не с cifar10, а с MNIST, CIFAR100 и IMAGENET.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE63AUx6pns6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OLpxasXqE6n"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FQq0ZxIw6Sx"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(' Learning rate: {:10.9f}'.format(self.model.optimizer._decayed_lr(tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "78xuePM8xI4v",
    "outputId": "7fd89358-a263-41d5-cc72-485e768aa70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "epwc8CRFUOj8",
    "outputId": "daa3fbf0-90af-494b-f390-bf10d36636d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 тренировочные примеры\n",
      "10000 тестовые примеры\n",
      "Использование data augmentation в реальном времени\n",
      "Epoch 1/80\n",
      "98/98 [==============================] - 35s 359ms/step - loss: 2.0242 - accuracy: 0.3389 - val_loss: 3.3975 - val_accuracy: 0.1000\n",
      "Epoch 2/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 1.5113 - accuracy: 0.4663 - val_loss: 2.9516 - val_accuracy: 0.1161\n",
      "Epoch 3/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 1.3054 - accuracy: 0.5354 - val_loss: 2.8125 - val_accuracy: 0.2044\n",
      "Epoch 4/80\n",
      "98/98 [==============================] - 35s 353ms/step - loss: 1.1498 - accuracy: 0.5913 - val_loss: 2.2615 - val_accuracy: 0.2631\n",
      "Epoch 5/80\n",
      "98/98 [==============================] - 34s 346ms/step - loss: 1.0465 - accuracy: 0.6301 - val_loss: 1.3073 - val_accuracy: 0.5432\n",
      "Epoch 6/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.9797 - accuracy: 0.6539 - val_loss: 0.8839 - val_accuracy: 0.6909\n",
      "Epoch 7/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.9162 - accuracy: 0.6782 - val_loss: 0.8516 - val_accuracy: 0.7050\n",
      "Epoch 8/80\n",
      "98/98 [==============================] - 34s 346ms/step - loss: 0.8703 - accuracy: 0.6931 - val_loss: 0.8926 - val_accuracy: 0.6941\n",
      "Epoch 9/80\n",
      "98/98 [==============================] - 34s 347ms/step - loss: 0.8393 - accuracy: 0.7059 - val_loss: 0.8968 - val_accuracy: 0.6957\n",
      "Epoch 10/80\n",
      "98/98 [==============================] - 34s 347ms/step - loss: 0.8000 - accuracy: 0.7205 - val_loss: 0.7189 - val_accuracy: 0.7519\n",
      "Epoch 11/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.7776 - accuracy: 0.7320 - val_loss: 0.7346 - val_accuracy: 0.7442\n",
      "Epoch 12/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.7476 - accuracy: 0.7394 - val_loss: 0.7146 - val_accuracy: 0.7505\n",
      "Epoch 13/80\n",
      "98/98 [==============================] - 35s 354ms/step - loss: 0.7232 - accuracy: 0.7459 - val_loss: 0.7122 - val_accuracy: 0.7563\n",
      "Epoch 14/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.7070 - accuracy: 0.7543 - val_loss: 0.6508 - val_accuracy: 0.7744\n",
      "Epoch 15/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.6940 - accuracy: 0.7605 - val_loss: 0.6455 - val_accuracy: 0.7761\n",
      "Epoch 16/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.6748 - accuracy: 0.7663 - val_loss: 0.6349 - val_accuracy: 0.7781\n",
      "Epoch 17/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.6587 - accuracy: 0.7731 - val_loss: 0.6192 - val_accuracy: 0.7855\n",
      "Epoch 18/80\n",
      "98/98 [==============================] - 34s 346ms/step - loss: 0.6428 - accuracy: 0.7776 - val_loss: 0.5827 - val_accuracy: 0.7992\n",
      "Epoch 19/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.6308 - accuracy: 0.7823 - val_loss: 0.5706 - val_accuracy: 0.8016\n",
      "Epoch 20/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.6139 - accuracy: 0.7881 - val_loss: 0.6434 - val_accuracy: 0.7837\n",
      "Epoch 21/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.6086 - accuracy: 0.7907 - val_loss: 0.5882 - val_accuracy: 0.7983\n",
      "Epoch 22/80\n",
      "98/98 [==============================] - 34s 348ms/step - loss: 0.5966 - accuracy: 0.7962 - val_loss: 0.5924 - val_accuracy: 0.8000\n",
      "Epoch 23/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.5877 - accuracy: 0.7974 - val_loss: 0.5457 - val_accuracy: 0.8115\n",
      "Epoch 24/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.5778 - accuracy: 0.8016 - val_loss: 0.5770 - val_accuracy: 0.8064\n",
      "Epoch 25/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.5666 - accuracy: 0.8057 - val_loss: 0.5669 - val_accuracy: 0.8110\n",
      "Epoch 26/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.5629 - accuracy: 0.8045 - val_loss: 0.5543 - val_accuracy: 0.8129\n",
      "Epoch 27/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.5549 - accuracy: 0.8115 - val_loss: 0.5311 - val_accuracy: 0.8199\n",
      "Epoch 28/80\n",
      "98/98 [==============================] - 33s 339ms/step - loss: 0.5441 - accuracy: 0.8117 - val_loss: 0.5386 - val_accuracy: 0.8190\n",
      "Epoch 29/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.5370 - accuracy: 0.8149 - val_loss: 0.5067 - val_accuracy: 0.8304\n",
      "Epoch 30/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.5317 - accuracy: 0.8179 - val_loss: 0.5043 - val_accuracy: 0.8288\n",
      "Epoch 31/80\n",
      "98/98 [==============================] - 34s 348ms/step - loss: 0.5282 - accuracy: 0.8189 - val_loss: 0.5033 - val_accuracy: 0.8313\n",
      "Epoch 32/80\n",
      "98/98 [==============================] - 34s 346ms/step - loss: 0.5166 - accuracy: 0.8231 - val_loss: 0.5039 - val_accuracy: 0.8268\n",
      "Epoch 33/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.5170 - accuracy: 0.8229 - val_loss: 0.5187 - val_accuracy: 0.8265\n",
      "Epoch 34/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.5135 - accuracy: 0.8224 - val_loss: 0.4892 - val_accuracy: 0.8335\n",
      "Epoch 35/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.5061 - accuracy: 0.8261 - val_loss: 0.5198 - val_accuracy: 0.8260\n",
      "Epoch 36/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.5021 - accuracy: 0.8262 - val_loss: 0.4833 - val_accuracy: 0.8364\n",
      "Epoch 37/80\n",
      "98/98 [==============================] - 33s 339ms/step - loss: 0.4939 - accuracy: 0.8304 - val_loss: 0.4961 - val_accuracy: 0.8323\n",
      "Epoch 38/80\n",
      "98/98 [==============================] - 33s 338ms/step - loss: 0.4944 - accuracy: 0.8302 - val_loss: 0.4706 - val_accuracy: 0.8423\n",
      "Epoch 39/80\n",
      "98/98 [==============================] - 33s 339ms/step - loss: 0.4919 - accuracy: 0.8311 - val_loss: 0.5000 - val_accuracy: 0.8326\n",
      "Epoch 40/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.4858 - accuracy: 0.8343 - val_loss: 0.4900 - val_accuracy: 0.8338\n",
      "Epoch 41/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4892 - accuracy: 0.8321 - val_loss: 0.5233 - val_accuracy: 0.8260\n",
      "Epoch 42/80\n",
      "98/98 [==============================] - 33s 338ms/step - loss: 0.4796 - accuracy: 0.8348 - val_loss: 0.4826 - val_accuracy: 0.8382\n",
      "Epoch 43/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4772 - accuracy: 0.8358 - val_loss: 0.4929 - val_accuracy: 0.8336\n",
      "Epoch 44/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4740 - accuracy: 0.8354 - val_loss: 0.4571 - val_accuracy: 0.8465\n",
      "Epoch 45/80\n",
      "98/98 [==============================] - 33s 336ms/step - loss: 0.4708 - accuracy: 0.8392 - val_loss: 0.4739 - val_accuracy: 0.8408\n",
      "Epoch 46/80\n",
      "98/98 [==============================] - 33s 339ms/step - loss: 0.4708 - accuracy: 0.8407 - val_loss: 0.4491 - val_accuracy: 0.8497\n",
      "Epoch 47/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.4657 - accuracy: 0.8385 - val_loss: 0.4713 - val_accuracy: 0.8411\n",
      "Epoch 48/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4675 - accuracy: 0.8419 - val_loss: 0.4620 - val_accuracy: 0.8436\n",
      "Epoch 49/80\n",
      "98/98 [==============================] - 33s 342ms/step - loss: 0.4616 - accuracy: 0.8411 - val_loss: 0.4737 - val_accuracy: 0.8423\n",
      "Epoch 50/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.4614 - accuracy: 0.8427 - val_loss: 0.4635 - val_accuracy: 0.8421\n",
      "Epoch 51/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.4536 - accuracy: 0.8447 - val_loss: 0.4669 - val_accuracy: 0.8422\n",
      "Epoch 52/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4576 - accuracy: 0.8426 - val_loss: 0.4710 - val_accuracy: 0.8426\n",
      "Epoch 53/80\n",
      "98/98 [==============================] - 33s 339ms/step - loss: 0.4546 - accuracy: 0.8449 - val_loss: 0.4488 - val_accuracy: 0.8486\n",
      "Epoch 54/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.4568 - accuracy: 0.8427 - val_loss: 0.4553 - val_accuracy: 0.8448\n",
      "Epoch 55/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.4543 - accuracy: 0.8438 - val_loss: 0.4408 - val_accuracy: 0.8512\n",
      "Epoch 56/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.4513 - accuracy: 0.8450 - val_loss: 0.4588 - val_accuracy: 0.8439\n",
      "Epoch 57/80\n",
      "98/98 [==============================] - 33s 335ms/step - loss: 0.4454 - accuracy: 0.8468 - val_loss: 0.4448 - val_accuracy: 0.8515\n",
      "Epoch 58/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.4489 - accuracy: 0.8460 - val_loss: 0.4566 - val_accuracy: 0.8455\n",
      "Epoch 59/80\n",
      "98/98 [==============================] - 34s 347ms/step - loss: 0.4455 - accuracy: 0.8448 - val_loss: 0.4671 - val_accuracy: 0.8447\n",
      "Epoch 60/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.4450 - accuracy: 0.8480 - val_loss: 0.4592 - val_accuracy: 0.8458\n",
      "Epoch 61/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.4433 - accuracy: 0.8480 - val_loss: 0.4532 - val_accuracy: 0.8463\n",
      "Epoch 62/80\n",
      "98/98 [==============================] - 34s 346ms/step - loss: 0.4472 - accuracy: 0.8472 - val_loss: 0.4464 - val_accuracy: 0.8505\n",
      "Epoch 63/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.4382 - accuracy: 0.8491 - val_loss: 0.4484 - val_accuracy: 0.8505\n",
      "Epoch 64/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.4408 - accuracy: 0.8493 - val_loss: 0.4555 - val_accuracy: 0.8484\n",
      "Epoch 65/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.4408 - accuracy: 0.8478 - val_loss: 0.4603 - val_accuracy: 0.8454\n",
      "Epoch 66/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.4406 - accuracy: 0.8483 - val_loss: 0.4574 - val_accuracy: 0.8475\n",
      "Epoch 67/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.4386 - accuracy: 0.8503 - val_loss: 0.4511 - val_accuracy: 0.8495\n",
      "Epoch 68/80\n",
      "98/98 [==============================] - 33s 342ms/step - loss: 0.4334 - accuracy: 0.8520 - val_loss: 0.4613 - val_accuracy: 0.8482\n",
      "Epoch 69/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.4416 - accuracy: 0.8482 - val_loss: 0.4425 - val_accuracy: 0.8517\n",
      "Epoch 70/80\n",
      "98/98 [==============================] - 33s 337ms/step - loss: 0.4372 - accuracy: 0.8484 - val_loss: 0.4515 - val_accuracy: 0.8476\n",
      "Epoch 71/80\n",
      "98/98 [==============================] - 33s 338ms/step - loss: 0.4318 - accuracy: 0.8512 - val_loss: 0.4463 - val_accuracy: 0.8495\n",
      "Epoch 72/80\n",
      "98/98 [==============================] - 33s 341ms/step - loss: 0.4297 - accuracy: 0.8533 - val_loss: 0.4496 - val_accuracy: 0.8497\n",
      "Epoch 73/80\n",
      "98/98 [==============================] - 34s 343ms/step - loss: 0.4296 - accuracy: 0.8515 - val_loss: 0.4450 - val_accuracy: 0.8504\n",
      "Epoch 74/80\n",
      "98/98 [==============================] - 33s 340ms/step - loss: 0.4347 - accuracy: 0.8509 - val_loss: 0.4472 - val_accuracy: 0.8502\n",
      "Epoch 75/80\n",
      "98/98 [==============================] - 33s 342ms/step - loss: 0.4326 - accuracy: 0.8524 - val_loss: 0.4499 - val_accuracy: 0.8492\n",
      "Epoch 76/80\n",
      "98/98 [==============================] - 34s 348ms/step - loss: 0.4310 - accuracy: 0.8541 - val_loss: 0.4439 - val_accuracy: 0.8527\n",
      "Epoch 77/80\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 0.4266 - accuracy: 0.8531 - val_loss: 0.4446 - val_accuracy: 0.8506\n",
      "Epoch 78/80\n",
      "98/98 [==============================] - 34s 347ms/step - loss: 0.4279 - accuracy: 0.8518 - val_loss: 0.4487 - val_accuracy: 0.8507\n",
      "Epoch 79/80\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 0.4268 - accuracy: 0.8529 - val_loss: 0.4423 - val_accuracy: 0.8540\n",
      "Epoch 80/80\n",
      "98/98 [==============================] - 34s 344ms/step - loss: 0.4281 - accuracy: 0.8528 - val_loss: 0.4515 - val_accuracy: 0.8498\n",
      "сохранить обученную модель как /content/saved_models/keras_cifar10_trained_model.h5 \n",
      "  544/10000 [>.............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 219us/step\n",
      "Test loss: 0.4515459008216858\n",
      "Test accuracy: 0.8497999906539917\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow import keras # расскоментируйте эту строку, чтобы начать обучение\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 512\n",
    "num_classes = 10\n",
    "epochs = 80\n",
    "l_2_reg = 1e-5\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# разделение тренировочной и тестовой выборки\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'тренировочные примеры')\n",
    "print(x_test.shape[0], 'тестовые примеры')\n",
    "\n",
    "# преобразование матрицы чисел 0-9 в бинарную матрицу чисел 0-1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "#opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.6)\n",
    "\n",
    "#opt= keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "# компиляция модели\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 - 0.5\n",
    "x_test /= 255 - 0.5\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False, \n",
    "        zca_epsilon=1e-06, \n",
    "        rotation_range=0, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0., \n",
    "        zoom_range=0., \n",
    "        channel_shift_range=0.,\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        rescale=None,\n",
    "        preprocessing_function=None,\n",
    "        data_format=None,\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # запуск data augmentation через fit\n",
    "    #datagen.fit(x_train)\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCXcJMwgi44l"
   },
   "outputs": [],
   "source": [
    "# Задание 1\n",
    "# 1. увеличение числа эпох до 30 повысило точность, но при этом алгоритм не успевал выходить на плато;\n",
    "# 2. learning_rate был увеличен в 10 раз с 0.0001 до 0.001, что увеличило скорость сходимости алгоритма. При \n",
    "# этом алгоритм за 30 эпох успевал выйти на плато;\n",
    "# 3. Adam увеличил скорость сходимости алгоритма, так как он позволяет быстрее выходить из седловых точек;\n",
    "# 4. добавление BatchNormalization() увеличило скорость сходимости алгоритма, так как данные приводятся к одному масштабу;\n",
    "# 5. удаление dropout слоев привело к переобучению: на тренировочных данных точность достигла 100% а на тестовых снизилась;\n",
    "# 6. применеие l_2 регуляризации не дало улучшений, видимо потому что dropout хорошо боролся с переобучением;\n",
    "# 7. применение learning rate sheduling повысило точность алгоритма, так как меньший шаг позволяет глубже погрузиться в \n",
    "# локальный минимум;\n",
    "# 8. увеличение размера батча уменьшает время, затрачиваемое на прохождение конкретной эпохи, но увеличивает требования \n",
    "# к объему оперативной памяти;\n",
    "# 9. добавление еще одного блока из 2-х сверточных слоев повысило точность сети (с одновременным уменьшением размера \n",
    "# полносвязного слоя), так как сверточная часть стала выявлять более абстрактные признаки;\n",
    "# 10. использование аугументации повысило точность и уменьшило переобучение, так как данные стали более разнообразными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2\n",
    "# 1. если бы данная нейронная сеть работала с MNIST, то ее сложность была бы избыточной, так как входные данные \n",
    "# относительно просты. Необходимо уменьшить количество слоев и фильтров, так как паттерны в изображениях менее разнообразны,\n",
    "# что повысит скорость работы и уменьшит вероятность переобучения;\n",
    "# 2. чтобы данная нейронная сеть работала с CIFAR-100, необходимо параметру num_classes присвоить значение 100, чтобы\n",
    "# последний полносвязный слой имел 100 выходов. Возможно потребуется больше фильтров, так как чем больше классов, тем больше\n",
    "# возможных высокоабстрактных признаков можно извлечь из изображений\n",
    "# 3. чтобы данная сеть работала с IMAGENET нужно провести больше всего изменений:\n",
    "# увеличить число слоев и фильтров, так как входные изображения имеют большее разрешение а также большее разнообразие;\n",
    "# параметру num_classes необходимо присвоить значение, равное количеству классов в датасете."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Intro_NN_homework_4_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
